[{"authors":null,"categories":null,"content":"Hi! I’m Pranav Balaji, a final year Computer Science undergraduate at BITS Pilani, Hyderabad. I’m mainly interested in the field of Natural Language Processing and Sequence Processing in general.\nI’ve worked on many Deep Learning projects in the past as part of my internship, course-work and my own interest. Some of them include Sign Language Translation, Paraphrase Generation and Recommender systems among others. What started off as a simple exploration of the field, when I did the Machine Learning course by Andrew Ng, is slowly growing into my professional research interest. PyTorch is my framework of choice.\nI’m also interested in Computer Graphics with one of my favourite projects being Ray-tracing implemented in OpenGL. Looking forward to working on something that lies in the intersection of CG and ML.\nFor fun, I love to do Origami, Speedcubing, and go Scuba diving - having recently gotten certified by PADI as an Advanced Open Water Diver. As of the past 3 years, I’ve also been learning Japanese and am fascinated by the culture. Definitely visiting there someday!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi! I’m Pranav Balaji, a final year Computer Science undergraduate at BITS Pilani, Hyderabad. I’m mainly interested in the field of Natural Language Processing and Sequence Processing in general.\nI’ve worked on many Deep Learning projects in the past as part of my internship, course-work and my own interest.","tags":null,"title":"Pranav Balaji","type":"authors"},{"authors":null,"categories":null,"content":"To better help bridge the gap between the hearing and speech impaired community and us, we sought to implement Sign Language Translation as part of our Machine Learning course.\nWhat most people think of when the problem is stated, is designing a model that takes in an image containing a static gesture being signed and outputs the “message” being contained in it. While it is definitely a solid first step, this is not at all representative of the real world problem.\nMost gestures in sign language are dynamic, i.e., the meaning is contained in movement. In essence, it is the non-trivial task of interpreting text sequences from video sequences. Moreover, like other translation problems, the input word order is usually not the same as the output word order either. Both of these reasons make this a sequence2sequence problem.\nOf course, we had to start somewhere. Using this kaggle dataset, which contains ASL images labelled with their corresponding alphabet, we fine-tuned the inception-v3 model with a simple classification task. This was fairly simple, and upon achieving satisfactory results we moved on to our main objective, video processing.\nWe decided to go ahead with a transformer encoder-decoder architecture. In the domain of NLP, it almost always has a word embedding layer that serves as a look-up table for fetching the appropriate embedding vector for each word in the input. These embeddings are what are then passed through deeper layer of the encoder blocks. In our case, however, there are no words in the input. We had to somehow “embed” frames of the videos. We can’t use a similar approach since image space is continuous while lexicon space isn’t. In other words, there are (well, almost, considering the input dimensions and the discreet RGB space) infinitely many frames that can exist.\nThe most natural way to embed images, is, well, through a CNN. We chose the original inception-v3 as in our previous experiment, and took the output from the penultimate layer as the embeddings. Transformers also require something called position encodings, so they were added before passing them in. As for the specific transformer architecture itself, we used the pre-trained T5-small (small, due to GPU constraints) so that we could leverage its existing German knowledge.\nWe tried different configurations of hyperparameters and ran training for ~1.5 hours per config. The best configuration gave us a test BLEU score of 12.75. While not the best, it isn’t a very representative metric as the labelled examples had only one translation per video, and BLEU’s representativity increases with more number of semantically equivalent labels. There’s also reason to believe that with a bigger transformer model and more data, the results would be much better, as is always the case with transformers. For more technical details, check out our report!\n","date":1650412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650412800,"objectID":"4fb20b0d7523ea608a9aae7897497adc","permalink":"https://greenfish8090.github.io/project/slt/","publishdate":"2022-04-20T00:00:00Z","relpermalink":"/project/slt/","section":"project","summary":"A transformer model that takes in video sequences signed in German Sign Language and translates them into readable German text","tags":["Deep Learning","Sequence Processing","Video Processing","Machine Translation"],"title":"Sign Language Translation","type":"project"},{"authors":null,"categories":null,"content":"In the domain of Natural Language Processing, it is unheard of for models to be trained on multiple tasks sequentially. This is because after the first task, there is a significant dip in performance on the first task while training for the second task. This is known as catastrophic forgetting. Continual learning aims to combat this problem by retaining knowledge of previous tasks while being able to adapt to any new task. We utilize a loss based method called Elastic Weights Consolidation and apply it on the T5 transformer to enable it to adapt to almost any NLP task while being fast and memory efficient.\nCheck out the slides, paper and the code on GitHub for more details!\n","date":1650412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650412800,"objectID":"aeba332f7a3b56003c889996fe589b78","permalink":"https://greenfish8090.github.io/project/paraphraser/","publishdate":"2022-04-20T00:00:00Z","relpermalink":"/project/paraphraser/","section":"project","summary":"The T5 multi-task transformer model pretrained on a wide array of tasks, fine-tuned for paraphrase generation while preserving performance on old tasks. Achieved with continual learning.","tags":["Deep Learning","Sequence Processing","Natural Language Processing","Continual Learning"],"title":"Paraphrase Generator","type":"project"},{"authors":null,"categories":null,"content":"We often find ourselves wondering what show/movie to watch next after we’ve just finished an amazing one. If it’s on Netflix, it always has something ready for us under the “similar users have liked…” section. But how does it do it exactly?\nNetflix, along with most popular content streaming sites, employ what’s called a recommender system with the primary goal of serving personalized recommendations to users. Broadly, based on its working, it can be classified into two types (Although in practice, it’s usually a mix of them):\nContent based filtering Collaborative filtering In content based filtering, items (products / movies / songs) are recommended solely based on the item’s description and your preference history. It tries to recommend items with descriptions similar to ones you’ve liked.\nOn the other hand, in collaborative filtering, the engine looks at the entire userbase of the app, and recommends items based on what others similar to you have liked. For this scheme, descriptions of items are not required. This is what I’ve implemented in this project.\nThe subset of the Dataset I used consists of just 3 columns: user_id, anime_id and rating. Arranging this into a matrix with rows as anime and columns as users, its cells can be filled with the rating (between 1 and 10) that the particular user rated the anime. An important feature of this resulting 5,000 x 100,000 matrix is that it’s sparse. Very, very sparse.\nRoughly only 4.5% of the matrix is filled. Now comes our herculean task of filling in the missing values. If we achieve this, our “filled-in” values can serve as our predictions of what those users would rate those anime.\nTo do this, matrix factorization can be used. The 5,000 x 100,000 dimensional sparse ratings matrix is decomposed into a 5,000 x 10 dimensional anime_matrix and a 10 x 100,000 dimensional user_matrix. These matrices are initialized at random, and are iteratively updated to converge them such that their product matches the original ratings matrix, wherever the ratings are available. After training, we simply multiply the two matrices and we have a whole array of predictions per user.\nI’ve tested it with my friends and myself, the predictions were rather accurate! For technical details and a walkthrough on how I went about implementing matrix factorization and generating predictions, check out the code on GitHub!\n","date":1650412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650412800,"objectID":"dd2043107cf5841e3e10cae4565e7107","permalink":"https://greenfish8090.github.io/project/recommender/","publishdate":"2022-04-20T00:00:00Z","relpermalink":"/project/recommender/","section":"project","summary":"A collaborative filtering model that recommends anime shows to users based on preferences of similar users","tags":["Deep Learning","Recommender system"],"title":"Anime Recommender System","type":"project"},{"authors":null,"categories":null,"content":"Ant Colony Optimization falls under the category of swarm intelligence algorithms, commonly used as a metaheuristic. It is, unsurprisingly, inspired from ants. The problem is formulated as a traversible field in which numerous ants are deployed. They follow a path dictated by the phermone levels and a problem-specific heuristic, in a probabilistic way. In this setting, the ants’ paths are possible solutions and depending upon the quality, pheromones are updated in the field for the next batch of ants to consider. After sufficient iterations, the best solutions are taken.\nAn adaptation of this system is the Ant Miner, a rule-based classifer which is commonly used for Data Mining tasks. This, along with an extension for handling continuous values called c-Ant Miner, is what was implemented in this project. The dataset used is the popular Ravdess Dataset for Emotion Recognition. Check out the code for details!\n","date":1650412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650412800,"objectID":"fca003340fed8aefa9ef610d0100c891","permalink":"https://greenfish8090.github.io/project/emotion-recog/","publishdate":"2022-04-20T00:00:00Z","relpermalink":"/project/emotion-recog/","section":"project","summary":"Emotion Recognition with a variation of Ant Colony Optimization for data mining on a preprocessed dataset","tags":["Data Mining","Bio-inspired algorithms"],"title":"Ant Miner","type":"project"},{"authors":[],"categories":[],"content":"Introduction Formalizing ‘forgetting’ Formalizing ‘forgetting’ (Cont.) Hypothesis Dataset Training setup Training setup (Cont.) The Original T5 T5 on standard fine-tuning T5 with EWC fine-tuning Results Graphs Graphs (Cont.) Graphs (Cont.) Discussion on results Conclusion References ","date":1633392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633392000,"objectID":"72cbbed3b139ae5606e1d868a43a2dd6","permalink":"https://greenfish8090.github.io/slides/paraphraser/","publishdate":"2021-10-05T00:00:00Z","relpermalink":"/slides/paraphraser/","section":"slides","summary":"Enabling the T5 model with continual learning using Elastic Weights Consolidation","tags":[],"title":"Continual Learning","type":"slides"}]