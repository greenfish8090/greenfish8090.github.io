[{"authors":null,"categories":null,"content":"Hi! I’m Pranav Balaji and I love building things.\nWhen I was little, it manifested as Origami, Lego and taking apart RC cars and helicopters only to assemble them back. As I grew older, I gravitated towards Computer Science and Engineering where I had seemingly limitless freedom on what I could do. I explored all the way from Arduino to Game dev to Web dev, but what stuck around was Machine Learning. I ended up doing my undergraduate thesis on it and co-founded a startup in the field.\nI’ve worked on many Deep Learning projects in the past as part of my education, work and my own interest - Talking head video generation, Video sign language translation, Continually trained transformer for paraphrase generation and an Anime recommender system are a few.\nMy fascination with Computer Graphics started off with a university project where my friends and I built a Ray Tracing pipeline with OpenGL and C++. This slowly evolved into a love for Computer Vision and I’ve been working in it ever since.\nFor fun, I (still) love to do Origami, Speedcube, keep minimally mobile pets and go Scuba diving - having the Advanced Open Water Diver certification. As of the past 5 years, I’ve also been learning Japanese intermittently and have a deep fascination with the language.\n","date":1696204800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1696204800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi! I’m Pranav Balaji and I love building things.\nWhen I was little, it manifested as Origami, Lego and taking apart RC cars and helicopters only to assemble them back. As I grew older, I gravitated towards Computer Science and Engineering where I had seemingly limitless freedom on what I could do.","tags":null,"title":"Pranav Balaji","type":"authors"},{"authors":null,"categories":null,"content":"To better help bridge the gap between the hearing and speech impaired community and us, we sought to implement Sign Language Translation as part of our Machine Learning course.\nWhat most people think of when the problem is stated, is designing a model that takes in an image containing a static gesture being signed and outputs the “message” being contained in it. While it is definitely a solid first step, this is not at all representative of the real world problem.\nMost gestures in sign language are dynamic, i.e., the meaning is contained in movement. In essence, it is the non-trivial task of interpreting text sequences from video sequences. Moreover, like other translation problems, the input word order is usually not the same as the output word order either. Both of these reasons make this a sequence2sequence problem.\nOf course, we had to start somewhere. Using this kaggle dataset, which contains ASL images labelled with their corresponding alphabet, we fine-tuned the inception-v3 model with a simple classification task. This was fairly simple, and upon achieving satisfactory results we moved on to our main objective, video processing.\nWe decided to go ahead with a transformer encoder-decoder architecture. In the domain of NLP, it almost always has a word embedding layer that serves as a look-up table for fetching the appropriate embedding vector for each word in the input. These embeddings are what are then passed through deeper layer of the encoder blocks. In our case, however, there are no words in the input. We had to somehow “embed” frames of the videos. We can’t use a similar approach since image space is continuous while lexicon space isn’t. In other words, there are (well, almost, considering the input dimensions and the discreet RGB space) infinitely many frames that can exist.\nThe most natural way to embed images, is, well, through a CNN. We chose the original inception-v3 as in our previous experiment, and took the output from the penultimate layer as the embeddings. Transformers also require something called position encodings so they can keep track of the temporal ordering of frames, so they were added before passing them in. As for the specific transformer architecture itself, we used the pre-trained T5-small (small, due to GPU constraints) so that we could leverage its existing German knowledge.\nWe tried different configurations of hyperparameters and ran training for ~1.5 hours per config. The best configuration gave us a test BLEU score of 12.75. While not the best, it isn’t a very representative metric as the labelled examples had only one translation per video, and BLEU’s representativity increases with more number of semantically equivalent labels. There’s also reason to believe that with a bigger transformer model and more data, the results would be much better, as is always the case with transformers. For more technical details, check out our report!\n","date":1650412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650412800,"objectID":"1ead8c4595f76226c2a9706ed3c8804a","permalink":"https://pranavbalaji.com/project/sign-language-translation/","publishdate":"2022-04-20T00:00:00Z","relpermalink":"/project/sign-language-translation/","section":"project","summary":"A transformer model that takes in video sequences signed in German Sign Language and translates them into readable German text","tags":["Deep Learning","Sequence Processing","Video Processing","Machine Translation","Natural Language Generation"],"title":"Sign Language Translation","type":"project"},{"authors":null,"categories":null,"content":"In the domain of Natural Language Processing, it is unheard of for models to be trained on multiple tasks sequentially. This is because after the first task, there is a significant dip in performance on the first task while training for the second task. This is known as catastrophic forgetting. Continual learning aims to combat this problem by retaining knowledge of previous tasks while being able to adapt to any new task. We utilize a loss based method called Elastic Weights Consolidation and apply it on the T5 transformer to enable it to adapt to almost any NLP task while being fast and memory efficient.\nCheck out the slides, paper and the code on GitHub for more details!\n","date":1638662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638662400,"objectID":"b3c8178f4d401a4e89fde667529ecc64","permalink":"https://pranavbalaji.com/project/paraphrase-generator/","publishdate":"2021-12-05T00:00:00Z","relpermalink":"/project/paraphrase-generator/","section":"project","summary":"The T5 multi-task transformer model pretrained on a wide array of tasks, fine-tuned for paraphrase generation while preserving performance on old tasks. Alleviated Catastrophic forgetting using EWC.","tags":["Deep Learning","Sequence Processing","Natural Language Processing","Natural Language Generation","Continual Learning"],"title":"Paraphrase Generator","type":"project"},{"authors":null,"categories":null,"content":"Introduction Raytracing is an advanced and computing-intensive method of generating graphics. Unlike traditional methods of generating objects in 3D space, this method can yield much more accurate and realistic results with the caviat of extremely high processing requirement. While the type of raytracing we have made is only suitable for still objects due to rendering times, some versions of ray tracing or pseudo raytracing exist that can be used for realtime simulations.\nInitial Construction We can start from the basics. Raytracing, in short, is the method of casting rays at each individual pixel on a ‘screen’ in front of a scene of objects in the hopes of it hitting an object. If it does, it retrieves the ‘data’ of the point of the object it hit and using that prints out a specific color on the screen. This is done for every single pixel present on screen. The result is therefore extremely accurate but incredibly time-wasting.\nDiagrammatic representation of raytracing This is a simple enough strategy to start off with. The complexity comes in the data required for the color of each pixel. The first issue is to detect whether a ray is actually hitting an object. This problem is solved using standard matrix geometrical calculations based on the normal of a triangle, which most objects are made out of. Of course, customized algorithms for different types of solids can be made similarly.\nDrawing a basic sphere with only hit detection; Background for display purposes Now this process becomes extremely intensive very quickly and will take too long per frame to render, so a way out of this predicament is to split the work into multiple threads. A compute shader can accomplish This but dividing up and workspace into multiple work groups based on your graphics card and work on each one in seperate threads. This speeds up the process exponentially and eases up the work done by the CPU in calculations. A compute shader is fairly simple to set up, and for the purposes of OpenGL, we make a quad texture that covers the entire window onto which a texture is rendered and the compute shader draws pixels onto the texture mentioned.\nNow this looks pretty ugly without a background, so we added a skybox. It is a bounding box around a scene that appears to the viewer as the sky and background. Essentially, it is an arbitrarily large cube containing a warped texture or image to simulate atmosphere. We found this one off the internet:\nSkybox Another task we can undertake in light bouncing. We can set up ‘mirror’ objects that almost purely reflect light and iteratively create origin and destination rays from one surface to another to mimic reflection. The pixel color intensity is continuously modified by each bounce by a certain value until a ‘bounce limit’ is reached. We can reach some impressive results with this alone. You can now see behind the camera as well. We gave the spheres a slight tint:\nReflection limit = 1 This image was generated only by reflecting each camera ray once and finding the object at that location. That’s why you see the spheres inside the reflections in solid color. Increase that limit, and you can see reflections inside reflections, and so on:\nReflection limit = 2 Reflection limit = 3 Purely reflective spheres with reflection limit = 5 Strongly tinted spheres with reflection limit = 5 Phororealistic Rendering Now, we can process some basic lighting, which can be as simple as checking for light bouncing off an object. If we were to process every light ray emitted from a light source and check for objects it hit it would be too much even for a compute shader to process, so a workaround is to process light in the opposite direction, i.e from the camera eye to the light source. By checking the angle between a light ray and the ray we use to trace objects we can achieve some sort of primitive lighting.\nPrimitive lighting model Now we are prepared to start applying James T Kajiya’s rendering equation to get better results for not just mirrors but all kinds of surfaces:\nRendering Equation The left hand side of the equation denotes the light coming out of a point x, and is eventually what we want. The first term on RHS is the light emitted. This would be 0 for most objects, non zero for light sources. The integral describes the light that the point accumulates from the surroundings. The integral is applied over the hemisphere around the point in question, in every direction. The first function right after the integral is the bidirectional reflectance distribution function, BRDF. This function takes as input an incoming direction and an outgoing direction and spits out a value that correlates the two. For example, for a perfect mirror, the BRDF will produce 0 for every (incoming, outgoing) pair except the ones that have their angle of reflection equal to angle of incidence. Other surfaces will have a more complicated correlation.\nCrude visualization of what the BRDF does Note that “incoming” in BRDF terminology …","date":1619395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619395200,"objectID":"e263c45cb96be1caf356bb5dfd1334ad","permalink":"https://pranavbalaji.com/project/ray-tracing/","publishdate":"2021-04-26T00:00:00Z","relpermalink":"/project/ray-tracing/","section":"project","summary":"An iterative ray tracer that renders 3D objects under realistic illumination","tags":["Computer Graphics","Ray tracing"],"title":"Ray Tracing","type":"project"},{"authors":null,"categories":null,"content":"We often find ourselves wondering what show/movie to watch next after we’ve just finished an amazing one. If it’s on Netflix, it always has something ready for us under the “similar users have liked…” section. But how does it do it exactly?\nNetflix, along with most popular content streaming sites, employ what’s called a recommender system with the primary goal of serving personalized recommendations to users. Broadly, based on its working, it can be classified into two types (Although in practice, it’s usually a mix of them):\nContent based filtering Collaborative filtering In content based filtering, items (products / movies / songs) are recommended solely based on the item’s description and your preference history. It tries to recommend items with descriptions similar to ones you’ve liked.\nOn the other hand, in collaborative filtering, the engine looks at the entire userbase of the app, and recommends items based on what others similar to you have liked. For this scheme, descriptions of items are not required. This is what I’ve implemented in this project.\nThe subset of the Dataset I used consists of just 3 columns: user_id, anime_id and rating. Arranging this into a matrix with rows as anime and columns as users, its cells can be filled with the rating (between 1 and 10) that the particular user rated the anime. An important feature of this resulting 5,000 x 100,000 matrix is that it’s sparse. Very, very sparse.\nRoughly only 4.5% of the matrix is filled. Now comes our herculean task of filling in the missing values. If we achieve this, our “filled-in” values can serve as our predictions of what those users would rate those anime.\nTo do this, matrix factorization can be used. The 5,000 x 100,000 dimensional sparse ratings matrix is decomposed into a 5,000 x 10 dimensional anime_matrix and a 10 x 100,000 dimensional user_matrix. These matrices are initialized at random, and are iteratively updated to converge them such that their product matches the original ratings matrix, wherever the ratings are available. After training, we simply multiply the two matrices and we have a whole array of predictions per user.\nI’ve tested it with my friends and myself, the predictions were rather accurate! For technical details and a walkthrough on how I went about implementing matrix factorization and generating predictions, check out the code on GitHub!\n","date":1637107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"86957cc4b775788eee4e2aaa6cda0c80","permalink":"https://pranavbalaji.com/project/anime-recommender-system/","publishdate":"2021-11-17T00:00:00Z","relpermalink":"/project/anime-recommender-system/","section":"project","summary":"A collaborative filtering model that recommends anime shows to users based on preferences of similar users","tags":["Deep Learning","Recommender system"],"title":"Anime Recommender System","type":"project"},{"authors":null,"categories":null,"content":"Ant Colony Optimization falls under the category of swarm intelligence algorithms, commonly used as a metaheuristic. It is, unsurprisingly, inspired from ants. The problem is formulated as a traversible field in which numerous ants are deployed. They follow a path dictated by the phermone levels and a problem-specific heuristic, in a probabilistic way. In this setting, the ants’ paths are possible solutions and depending upon the quality, pheromones are updated in the field for the next batch of ants to consider. After sufficient iterations, the best solutions are taken.\nAn adaptation of this system is the Ant Miner, a rule-based classifer which is commonly used for Data Mining tasks. This, along with an extension for handling continuous values called c-Ant Miner, is what was implemented in this project. The dataset used is the popular Ravdess Dataset for Emotion Recognition. Check out the code for details!\n","date":1638057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638057600,"objectID":"fca003340fed8aefa9ef610d0100c891","permalink":"https://pranavbalaji.com/project/emotion-recog/","publishdate":"2021-11-28T00:00:00Z","relpermalink":"/project/emotion-recog/","section":"project","summary":"Ant Miner, a variation of Ant Colony Optimization for data mining applied to a preprocessed emotion recognition dataset","tags":["Data Mining","Bio-inspired algorithms","Emotion recogniton"],"title":"Emotion Recognition","type":"project"},{"authors":["Pranav Balaji","Abhijit Das","Srijan Das","Antitza Dantcheva"],"categories":null,"content":"","date":1696204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696204800,"objectID":"3bbdc6155fe78198087710ddf6a12376","permalink":"https://pranavbalaji.com/publication/iccvw2023deepfake/","publishdate":"2023-10-02T00:00:00Z","relpermalink":"/publication/iccvw2023deepfake/","section":"publication","summary":"We explore various ways of using multi-task and contrastive learning to the problem of deepfake detection to improve generalizability.","tags":[],"title":"Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning","type":"publication"},{"authors":["Srijan Das","Tanmay Jain","Dominick Reilly","Pranav Balaji","Soumyajit Karmakar","Shyam Marjit","Xiang Li","Abhijit Das","Michael S Ryoo"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"cf63bd87017021824533e205391fa739","permalink":"https://pranavbalaji.com/publication/wacv2024data/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/wacv2024data/","section":"publication","summary":"This paper shows that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task is surprisingly beneficial when the amount of training data is limited.","tags":[],"title":"Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders","type":"publication"},{"authors":[],"categories":[],"content":"Introduction Formalizing ‘forgetting’ Formalizing ‘forgetting’ (Cont.) Hypothesis Dataset Training setup Training setup (Cont.) The Original T5 T5 on standard fine-tuning T5 with EWC fine-tuning Results Graphs Graphs (Cont.) Graphs (Cont.) Discussion on results Conclusion References ","date":1633392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633392000,"objectID":"72cbbed3b139ae5606e1d868a43a2dd6","permalink":"https://pranavbalaji.com/slides/paraphraser/","publishdate":"2021-10-05T00:00:00Z","relpermalink":"/slides/paraphraser/","section":"slides","summary":"Enabling the T5 model with continual learning using Elastic Weights Consolidation","tags":[],"title":"Continual Learning","type":"slides"}]